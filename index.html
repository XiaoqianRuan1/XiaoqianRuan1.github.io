<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Xiaoqian Ruan</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background: #f9f9f9;
      color: #333;
    }
    header {
      background: none;
      color: #000000;
      padding: 2rem;
      text-align: center;
    }
    .section {
      padding: 2rem;
      max-width: 900px;
      margin: auto;
    }
    .projects {
      display: flex;
      flex-wrap: nowrap;  
      gap: 1.5rem;
      overflow-x: auto; 
      padding-bottom: 1rem;
    }
    .project {
      background: white;
      border-radius: 8px;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
      padding: 1rem;
    }
    .project img {
      width: 100%;
      height: auto;
      border-radius: 4px;
    }
    footer {
      text-align: center;
      padding: 1rem;
      background: #eee;
      margin-top: 2rem;
    }
    a {
      color: #3498db;
      text-decoration: none;
    }
  .profile-container {
  display: flex;
  align-items: center;
  justify-content: center;
  flex-wrap: wrap;
  max-width: 1000px;
  margin: auto;
}

.profile-pic {
  width: 180px;
  height: 180px;
  border-radius: 50%;
  object-fit: cover;
  margin-right: 2rem;
  border: none;
  box-shadow: 0 0 10px rgba(0,0,0,0.2);
}

.profile-intro {
  max-width: 600px;
  text-align: left;
}
.contact-links {
  display: flex;
  gap: 1.5rem;
  justify-content: center;
  flex-wrap: wrap;
  margin-top: 1rem;
}

.contact-links a {
  text-decoration: none;
  color: #007acc;
  font-weight: bold;
}

.pub-list li {
  position: relative;
  margin: 0 0 0.8rem 0;
  line-height: 1.4;
  padding-left: 1.25rem;  /* indent for numbers/bullets */
}
    
.pub-list {
  list-style: none;
  padding: 0;
  margin: 0.8rem 0;
}

.pub-list li::before{
  content: "‚Ä¢";              /* or use "" and draw a dot with width/height */
  position: absolute;
  left: 0;
  top: 0.1em;                /* vertical align with first line */
  font-size: 1rem;           /* tweak size to taste */
  line-height: 1;  
} 
    
.research-card {
  display: flex;
  align-items: center;
  background-color: none;
  border-radius: 0;
  box-shadow: none;
  margin-bottom: 1.5rem;
  padding: 0;
}

.research-card img {
  height: 180px;
  width: 240px;             
  object-fit: contain;         /* crop nicely without stretching */
  border-radius: 8px;
  margin-right: 1.5rem;
  flex-shrink: 0;
}

.research-info {
  flex: 1;
}

.research-info h3 {
  margin: 0 0 0.5rem;
}
.research-links {
  margin-top: 0.5rem;
  display: flex;
  gap: 1rem;
  flex-wrap: wrap;
}

.research-links a {
  text-decoration: none;
  color: #007acc;
  font-weight: 500;
}

.research-links a:hover {
  text-decoration: underline;
}
  </style>
</head>
<body>

<header>
  <header>
  <div class="profile-container">
    <img src="images/faces.jpg" alt="Xiaoqian Ruan" class="profile-pic">
    <div class="profile-intro">
    <h1>Xiaoqian Ruan</h1>
      <p>
        I am Xiaoqian Ruan, a PhD student in the <a href="https://www.evl.uic.edu/" target="_black"> EVL lab</a> at University of Illinois Chicago, advised by <a href="https://www.cs.uic.edu/~tangw/" target="_black"> Prof. Wei Tang</a>. 
      </p>
      <p>
        My research focuses on 3D reconstruction, Test-time Adaptation, and Object detection.  
      </p>
      <div class="content-links">
        üìß <a href="mailto:xruan9@uic.edu">Email</a> 
        üíº <a href="https://www.linkedin.com/in/xiaoqian-ruan-b870682a4/" target="_blank">LinkedIn</a> 
        üê± <a href="https://github.com/XiaoqianRuan1" target="_blank">GitHub</a> 
        üìö <a href="https://scholar.google.com/citations?hl=en&user=q-sYRlgAAAAJ" target="_blank">Google Scholar</a>
    </div>
  </div>
</header>
</header>

<section class="section">
  <h2>Publications</h2>
  <ol class="pub-list">
    <li>
      <strong>Xiaoqian Ruan</strong>, Pei Yu, Dian Jia, Hyeonjeong Park, Peixi Xiong, Wei Tang.  
      <em>Learning Partonomic 3D Reconstruction from Image Collections</em>.  
      <strong>CVPR 2025</strong>.  
      [<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Ruan_Learning_Partonomic_3D_Reconstruction_from_Image_Collections_CVPR_2025_paper.pdf" target="_blank">üìÑ Paper</a>]  
    </li>
    <li>
      Dian Jia, <strong>Xiaoqian Ruan</strong>, Kun Xia, Zhiming Zou, Le Wang, Wei Tang.  
      <em>Analysis-by-Synthesis Transformer for Single-View 3D Reconstruction</em>.  
      <strong>ECCV 2024</strong>.  
      [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03170.pdf" target="_blank">üìÑ Paper</a>]  
    </li>
    <li>
      <strong>Xiaoqian Ruan</strong>, Wei Tang.  
      <em>Fully Test-time Adaptation for Object Detection</em>.  
      <strong>CVPRW 2024</strong>.  
      [<a href="https://openaccess.thecvf.com/content/CVPR2024W/MAT/papers/Ruan_Fully_Test-time_Adaptation_for_Object_Detection_CVPRW_2024_paper.pdf" target="_blank">üìÑ Paper</a>]  
    </li>
    <li>
      <strong>Xiaoqian Ruan</strong>, Guosheng Lin, Cheng Long, and Shengli Lu. 
      <em>Few-shot fine-grained classification with Spatial Attentive Comparision</em>.
      <strong>Knowledge-Based Systems 218 (2021)</strong>.
      [<a href="https://www.sciencedirect.com/science/article/pii/S0950705121001039" target="_blank">üìÑ Paper</a>]
    </li>
    <li>
      <strong>Xiaoqian Ruan</strong>, Hao Liu, Wei Pang, and Shengli Lu. 
      <em>Fine-gained Classification Algorithm based on Meta-learning</em>.
      2019 IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS).
      [<a href="https://ieeexplore.ieee.org/abstract/document/8942415" target="_blank">üìÑ Paper</a>]
    </li>
  </ol>
</section>
    
  
<section class="section">
  <h2>Selected Research</h2>
    <div class="research-card">
      <img src="images/part.png" alt="Project Image">
      <div class="research-info">
      <h3>Learning Partonomic 3D Reconstruction from Image Collections</h3>
      <p><strong>Xiaoqian Ruan</strong>, Pei Yu, Dian Jia, Hyeonjeong Park, Peixi Xiong, Wei Tang</p>
      <p>CVPR 2025</p>
      <div class="research-links">
        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Ruan_Learning_Partonomic_3D_Reconstruction_from_Image_Collections_CVPR_2025_paper.pdf" target="_blank">üìÑ Paper</a>
        <a href="https://github.com/XiaoqianRuan1/Partonomic_Reconstruction" target="_blank">üíª Code</a>
      </div>
    </div>
    </div>
    <div class="research-card">
      <img src="images/new_analysis.png" alt="Project Image">
      <div class="research-info">
      <h3>Analysis-by-Synthesis Transformer for Single-View 3D Reconstruction</h3>
      <p>Dian Jia, <strong>Xiaoqian Ruan</strong>, Kun Xia, Zhiming Zou, Le Wang, Wei Tang</p>
      <p>ECCV 2024</p>
      <div class="research-links">
        <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03170.pdf" target="_blank">üìÑ Paper</a>
      </div>
    </div>
    </div>
    <div class="research-card">
      <img src="images/test.png" alt="Project Image">
      <div class="research-info">
      <h3>Fully Test-time Adaptation for Object Detection</h3>
      <p><strong>Xiaoqian Ruan</strong>, Wei Tang</p>
      <p>CVPRW 2024</p>
      <div class="research-links">
        <a href="https://openaccess.thecvf.com/content/CVPR2024W/MAT/papers/Ruan_Fully_Test-time_Adaptation_for_Object_Detection_CVPRW_2024_paper.pdf" target="_blank">üìÑ Paper</a>
        <a href="https://github.com/XiaoqianRuan1/IoU-filter" target="_blank">üíª Code</a>
      </div>
    </div>
    </div>
    <div class="research-card">
      <img src="images/disjoint.png" alt="Project Image">
      <div class="research-info">
      <h3>Disjoint Contrastive Regression Learning for Multi-Sourced Annotations</h3>
      <p><strong>Xiaoqian Ruan</strong>, Gaoang Wang</p>
      <div class="research-links">
        <a href="https://arxiv.org/pdf/2112.15411" target="_blank">üìÑ Paper</a>
      </div>
    </div>
    </div>
    <div class="research-card">
        <img src="images/few.png" alt="Project Image">
        <div class="research-info">
        <h3>Few-shot fine-grained classification with Spatial Attentive Comparison</h3>
        <p><strong>Xiaoqian Ruan</strong>, Guosheng Lin, Cheng Long, Shengli Lu</p>
        <p>Knowledge-Based Systems</p>
        <div class="research-links">
          <a href="https://pdf.sciencedirectassets.com/271505/1-s2.0-S0950705121X00056/1-s2.0-S0950705121001039/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQC5TrO0vg8V23hc%2BbPf%2FPse%2Bdv8WDoc7P2leQrCpXPUJgIhALNW7afyu%2BSwh8IvdBTC4ndpUJ%2B9nPwMDD7FbDbnAJK1KrsFCIT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBRoMMDU5MDAzNTQ2ODY1Igx0ov97zJ87c54na9AqjwWd7DA4dRKzUJ3YqR7m%2BubNmRfVzg1VV4Tm3g%2FrxRD1At9NivJuhlQlR%2BB5b7Wk0nlNEAsA3pBKHW8QrZ%2F3dPsvvYVYgmMBPKBOak88Cp4GIG1Pg69YDp6VdF%2BzPdgfj1Dg1SnP5DmhBEbAqSVFgEn%2FhOJeRhu%2FkFogXo2lqaaRj0uFasUef8OOVmOWggwER9GBMy3qDuC2xqMTlYjRmqfbdQ9Z59MYLo%2BpsVy%2BhdkHjgpofaCtEjaiKFft4586OFx0nxhxA2Hrj9zvXEY6XirOJ2cTTL%2FCK%2B8ST%2F5U2Ile%2BDYUOMyUAhPbt5Up2pDBaRkKPTMDCdPSBIZFnQ%2FAUALXcnH9FTOsmstKbqiZGrnN%2F4%2BcSddI5SI0ocNyBLYhqkRsbI8Ih%2B7fiVf8f1hSuTK32VcKejWQACOEff7IfvZasMHMLoVlMqvoLBkbBh3QIpS377OdYI%2F4M%2F%2FOg8diBMfr2wLEph%2FCNgnKXk2TBLMvAFq1CP0MyRHv6%2Ft0S5aU4WrNp%2FwN6J7rSw5%2F5NWvZSlplMgf%2Fjrqx5u2smj5r9Ii%2FXxgeCM%2F012R75rTN6xofRKLdnxueWW85VlrIx8zjNhNRr4%2FNsHF8ekxWjU56aK%2FpJAdpPGQf7dDD%2BtUfqpO9STBXYy0hyqJZOLjJvVnFkfwABzvWipL%2FJzPRYRouCx3OpYNnRAqe86mLs6vQly%2B3NimRH6ERzc%2Bs9mIi0SzCpKLEAFI4XVmOONoWQp%2BE9q7evye3xR3FfzQ6aO5WewPGj4GnUZWIqzUet8KXmlJYxhRATtDrs9et1pKwAXhBeUtSwnMO6f9JvilX8%2FKcIjYHahv7WOWVlKriHtjbIjOyM2glT3N4gD1KISca8smTAmfMMnayMIGOrABCsCluJYEZfOLz4N%2FiBJrb1OzCailGjm52HJJ3Etx3nTCsHDgR8b8Ch8b9W3wFXY%2FwclKMK1a8i2INFSZR76CjahkpNHDD5vpy2dwv2D82CYRdKzta2d9aFHGbtvDXMwxNEAIMS58jpcK%2F9FM6Kh6eAImAXP9N7%2BSWAVlxzVAbxeiocjZXazuSVOInYDNhescqc4f4u%2BN4WA9wcU7MTvEzVn6tEFlyseQ5PJ2m%2BsJQM4%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250618T035432Z&X-Amz-SignedHeaders=host&X-Amz-Expires=299&X-Amz-Credential=ASIAQ3PHCVTY6QVQ7IDH%2F20250618%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=59de3cf68fa247873a5669a2a65307cce25c0e504f66ae1acef612ee3eb65507&hash=405b04efec18a69921c10395f4d25e6834ca7b00ae3ba325b4eea100d64947e7&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0950705121001039&tid=spdf-01cd1898-e9b1-4841-848c-c35050dc2f83&sid=ca2e1f254331034d3f89ec21b02e64d40acfgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0f155d535557035c5b5057&rr=9517d8904a5ceb62&cc=us" target="_blank">üìÑ Paper</a>
        </div>
      </div>
      </div>
</section>

</body>
</html>
